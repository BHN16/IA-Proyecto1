{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, zero_one_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y normalización de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataset = pd.read_csv('./data/Cardiotocographic-Training.csv')\n",
    "y = dataset.CLASE.to_numpy()\n",
    "X = dataset.drop('CLASE', axis=1).to_numpy()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootsrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y, model, k, c, g):\n",
    "    indices = np.array([i for i in range (len(X))])\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for i in range(k):\n",
    "        train_index = resample(indices, n_samples=k, replace=True)\n",
    "        test_index = np.array([j for j in indices if j not in train_index])\n",
    "        \n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X, y, model, k, c, g):\n",
    "    skf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo de modelo SVM con bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c y g:  1 0.1 \n",
      "bias:  0.1439361675105836 \n",
      "varianza:  0.00030112760334526794\n",
      "c y g:  1 0.01 \n",
      "bias:  0.1423274739312672 \n",
      "varianza:  0.0003072605206073213\n",
      "c y g:  1 0.001 \n",
      "bias:  0.14342937459374375 \n",
      "varianza:  0.00026729563538479623\n",
      "c y g:  10 0.1 \n",
      "bias:  0.12781933462177456 \n",
      "varianza:  7.859097853899336e-05\n",
      "c y g:  10 0.01 \n",
      "bias:  0.12791266926001227 \n",
      "varianza:  9.324019726630779e-05\n",
      "c y g:  10 0.001 \n",
      "bias:  0.12736541890552758 \n",
      "varianza:  9.475980371313346e-05\n",
      "c y g:  100 0.1 \n",
      "bias:  0.1331621553717318 \n",
      "varianza:  0.0001572181068333326\n",
      "c y g:  100 0.01 \n",
      "bias:  0.13209832375199287 \n",
      "varianza:  0.00016049068362795443\n",
      "c y g:  100 0.001 \n",
      "bias:  0.13148838596305595 \n",
      "varianza:  0.00011209861605542345\n"
     ]
    }
   ],
   "source": [
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results_svm_bootstrap = []\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results_svm_bootstrap.append(bootstrap(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), k, c, gama))\n",
    "\n",
    "# Valores de bias y varianza del respectivo hiperparametro\n",
    "for r in results_svm_bootstrap:\n",
    "    print(\"c y g: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de metricas del mejor modelo SVM con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8720873307399879\n",
      "recall:  0.8720873307399879\n",
      "f1:  0.8720873307399879\n",
      "auc:  0.9450841632767766\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", np.mean(results_svm_bootstrap[4][4]))\n",
    "print(\"recall: \", np.mean(results_svm_bootstrap[4][5]))\n",
    "print(\"f1: \", np.mean(results_svm_bootstrap[4][6]))\n",
    "print(\"auc: \", np.mean(results_svm_bootstrap[4][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = svm.SVC(kernel='linear', C=10, gamma=0.001, decision_function_shape='ovr', probability=True) # C es penalización por los valores fuera de la clasificación\n",
    "# gamma controla la distancia de influencia de un punto de entrenamiento, valor pequeño indica que el radio de distancia es más grande, por lo que mas puntos se agrupan correctamente.\n",
    "# valora alto implica que el radio se reduce y los puntos deben estar más cerca entre ellos para ser considerados del mismo grupo\n",
    "# precisions, recalls, f1s, aucs, errors = k_fold(X, y, clf, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo SVM con K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c y g:  1 0.1 \n",
      "bias:  0.11063567839195979 \n",
      "varianza:  0.00041917338577308693\n",
      "c y g:  1 0.01 \n",
      "bias:  0.11063567839195979 \n",
      "varianza:  0.00041917338577308693\n",
      "c y g:  1 0.001 \n",
      "bias:  0.11063567839195979 \n",
      "varianza:  0.00041917338577308693\n",
      "c y g:  10 0.1 \n",
      "bias:  0.1026256281407035 \n",
      "varianza:  0.000527016773566324\n",
      "c y g:  10 0.01 \n",
      "bias:  0.1026256281407035 \n",
      "varianza:  0.000527016773566324\n",
      "c y g:  10 0.001 \n",
      "bias:  0.1026256281407035 \n",
      "varianza:  0.000527016773566324\n",
      "c y g:  100 0.1 \n",
      "bias:  0.10212311557788945 \n",
      "varianza:  0.0005317254930431063\n",
      "c y g:  100 0.01 \n",
      "bias:  0.10212311557788945 \n",
      "varianza:  0.0005317254930431063\n",
      "c y g:  100 0.001 \n",
      "bias:  0.10212311557788945 \n",
      "varianza:  0.0005317254930431063\n"
     ]
    }
   ],
   "source": [
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results_svm_k = []\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results_svm_k.append(k_fold(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), 10, c, gama))\n",
    "\n",
    "# Valores de bias y varianza del respectivo hiperparametro\n",
    "for r in results_svm_k:\n",
    "    print(\"c y g: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de metricas del mejor modelo SVM con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8978768844221106\n",
      "recall:  0.8978768844221106\n",
      "f1:  0.8978768844221106\n",
      "auc:  0.9696444519793601\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", np.mean(results_svm_k[7][4]))\n",
    "print(\"recall: \", np.mean(results_svm_k[7][5]))\n",
    "print(\"f1: \", np.mean(results_svm_k[7][6]))\n",
    "print(\"auc: \", np.mean(results_svm_k[7][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo logística con K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty y c:  l1 10 \n",
      "bias:  0.10611557788944723 \n",
      "varianza:  0.00022118421251988624\n",
      "penalty y c:  l1 1 \n",
      "bias:  0.11162060301507534 \n",
      "varianza:  0.000407358431605263\n",
      "penalty y c:  l1 0.1 \n",
      "bias:  0.13915075376884423 \n",
      "varianza:  0.0005178608873513305\n",
      "penalty y c:  l2 10 \n",
      "bias:  0.10712311557788942 \n",
      "varianza:  0.0003048898070755793\n",
      "penalty y c:  l2 1 \n",
      "bias:  0.11212814070351759 \n",
      "varianza:  0.0005600506363475667\n",
      "penalty y c:  l2 0.1 \n",
      "bias:  0.16668341708542714 \n",
      "varianza:  0.000892777833893083\n"
     ]
    }
   ],
   "source": [
    "results_log_k = []\n",
    "penalty = ['l1', 'l2']\n",
    "cs = [10, 1, 0.1]\n",
    "\n",
    "for p in penalty:\n",
    "    for c in cs:\n",
    "        results_log_k.append(k_fold(X, y, LogisticRegression(penalty=p, C=c, multi_class='ovr', solver='liblinear', max_iter=1000), 10, p, c))\n",
    "\n",
    "for r in results_log_k:\n",
    "   print(\"penalty y c: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de métricas del mejor modelo logística con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8938844221105529\n",
      "recall:  0.8938844221105529\n",
      "f1:  0.8938844221105529\n",
      "auc:  0.9664880441174803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"precision: \", np.mean(results_log_k[0][4]))\n",
    "print(\"recall: \", np.mean(results_log_k[0][5]))\n",
    "print(\"f1: \", np.mean(results_log_k[0][6]))\n",
    "print(\"auc: \", np.mean(results_log_k[0][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo logística con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty y c:  l1 10 \n",
      "bias:  0.12552573073566517 \n",
      "varianza:  7.832044372455536e-05\n",
      "penalty y c:  l1 1 \n",
      "bias:  0.1488188010043481 \n",
      "varianza:  0.0002502422351480533\n",
      "penalty y c:  l1 0.1 \n",
      "bias:  0.2218754836281879 \n",
      "varianza:  1.0075781509611317e-05\n",
      "penalty y c:  l2 10 \n",
      "bias:  0.12922527920708887 \n",
      "varianza:  8.063018991392728e-05\n",
      "penalty y c:  l2 1 \n",
      "bias:  0.16701004947655654 \n",
      "varianza:  0.00020324513834105214\n",
      "penalty y c:  l2 0.1 \n",
      "bias:  0.22178458661514813 \n",
      "varianza:  1.0450366203825303e-05\n"
     ]
    }
   ],
   "source": [
    "results_log_bootstrap = []\n",
    "penalty = ['l1', 'l2']\n",
    "cs = [10, 1, 0.1]\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for p in penalty:\n",
    "    for c in cs:\n",
    "        results_log_bootstrap.append(bootstrap(X, y, LogisticRegression(penalty=p, C=c, multi_class='ovr', solver='liblinear', max_iter=1000), k, p, c))\n",
    "\n",
    "for r in results_log_bootstrap:\n",
    "   print(\"penalty y c: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de métricas del mejor modelo logística con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8744742692643348\n",
      "recall:  0.8744742692643348\n",
      "f1:  0.8744742692643348\n",
      "auc:  0.9461086742800177\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", np.mean(results_log_bootstrap[0][4]))\n",
    "print(\"recall: \", np.mean(results_log_bootstrap[0][5]))\n",
    "print(\"f1: \", np.mean(results_log_bootstrap[0][6]))\n",
    "print(\"auc: \", np.mean(results_log_bootstrap[0][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo DecisionTree con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterio y min_leaf:  gini 4 \n",
      "bias:  0.07808542713567838 \n",
      "varianza:  0.0007126785687230112\n",
      "criterio y min_leaf:  gini 6 \n",
      "bias:  0.07607035175879398 \n",
      "varianza:  0.0006983770864372101\n",
      "criterio y min_leaf:  gini 8 \n",
      "bias:  0.07908793969849245 \n",
      "varianza:  0.0005361430330042166\n",
      "criterio y min_leaf:  entropy 4 \n",
      "bias:  0.05056281407035178 \n",
      "varianza:  0.00019378591449710833\n",
      "criterio y min_leaf:  entropy 6 \n",
      "bias:  0.06856281407035175 \n",
      "varianza:  0.0004651291381530762\n",
      "criterio y min_leaf:  entropy 8 \n",
      "bias:  0.07557788944723617 \n",
      "varianza:  0.0001875984192318368\n"
     ]
    }
   ],
   "source": [
    "criterion = [\"gini\", \"entropy\"]\n",
    "min_leaf = [4, 6, 8]\n",
    "\n",
    "results_tree_k = []\n",
    "\n",
    "for c in criterion:\n",
    "    for l in min_leaf:\n",
    "        results_tree_k.append(k_fold(X, y, DecisionTreeClassifier(criterion=c, min_samples_leaf=l), 10, c, l))\n",
    "for r in results_tree_k:\n",
    "    print(\"criterio y min_leaf: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de metricas para el modelo DecisionTree con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.949437185929648\n",
      "recall:  0.949437185929648\n",
      "f1:  0.949437185929648\n",
      "auc:  0.9474398589743911\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", np.mean(results_tree_k[3][4]))\n",
    "print(\"recall: \", np.mean(results_tree_k[3][5]))\n",
    "print(\"f1: \", np.mean(results_tree_k[3][6]))\n",
    "print(\"auc: \", np.mean(results_tree_k[3][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo de DecisionTree con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterio y min_leaf:  gini 4 \n",
      "bias:  0.1412028926025946 \n",
      "varianza:  0.00040025944004300483\n",
      "criterio y min_leaf:  gini 6 \n",
      "bias:  0.14863084546150954 \n",
      "varianza:  0.0004789413708422947\n",
      "criterio y min_leaf:  gini 8 \n",
      "bias:  0.15285316795027007 \n",
      "varianza:  0.0003706799047221454\n",
      "criterio y min_leaf:  entropy 4 \n",
      "bias:  0.14235170108008824 \n",
      "varianza:  0.00038567857018375874\n",
      "criterio y min_leaf:  entropy 6 \n",
      "bias:  0.14965142518474675 \n",
      "varianza:  0.0004304999728206793\n",
      "criterio y min_leaf:  entropy 8 \n",
      "bias:  0.15743294367637262 \n",
      "varianza:  0.00046421599506013366\n"
     ]
    }
   ],
   "source": [
    "criterion = [\"gini\", \"entropy\"]\n",
    "min_leaf = [4, 6, 8]\n",
    "\n",
    "results_tree_bootstrap = []\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for c in criterion:\n",
    "    for l in min_leaf:\n",
    "        results_tree_bootstrap.append(bootstrap(X, y, DecisionTreeClassifier(criterion=c, min_samples_leaf=l), k, c, l))\n",
    "\n",
    "for r in results_tree_bootstrap:\n",
    "   print(\"criterio y min_leaf: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de métricas para el modelo DecisionTree con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8576482989199116\n",
      "recall:  0.8576482989199116\n",
      "f1:  0.8576482989199116\n",
      "auc:  0.8409504122166064\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", np.mean(results_tree_bootstrap[3][4]))\n",
    "print(\"recall: \", np.mean(results_tree_bootstrap[3][5]))\n",
    "print(\"f1: \", np.mean(results_tree_bootstrap[3][6]))\n",
    "print(\"auc: \", np.mean(results_tree_bootstrap[3][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción del modelo DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = pd.read_csv('./data/Cardiotocographic-Test.csv')\n",
    "x = dataset.to_numpy()\n",
    "x = scaler.fit_transform(x)\n",
    "dt = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=4)\n",
    "dt.fit(X, y)\n",
    "y_pred = dt.predict(x)\n",
    "\n",
    "text_file = open(\"predicts.txt\", \"w\")\n",
    "for l in y_pred:\n",
    "    text_file.write(str(l)+\"\\n\")\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
