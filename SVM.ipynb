{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, zero_one_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y normalización de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataset = pd.read_csv('./data/Cardiotocographic-Training.csv')\n",
    "y = dataset.CLASE.to_numpy()\n",
    "X = dataset.drop('CLASE', axis=1).to_numpy()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootsrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y, model, k, c, g):\n",
    "    indices = np.array([i for i in range (len(X))])\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for i in range(k):\n",
    "        train_index = resample(indices, n_samples=k, replace=True)\n",
    "        test_index = np.array([j for j in indices if j not in train_index])\n",
    "        \n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X, y, model, k, c, g):\n",
    "    skf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo de modelo SVM con bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c y g:  1 0.1 \n",
      "bias:  0.14375546484394902 \n",
      "varianza:  0.00029009612314725205\n",
      "c y g:  1 0.01 \n",
      "bias:  0.14267079651904602 \n",
      "varianza:  0.0003157299330377769\n",
      "c y g:  1 0.001 \n",
      "bias:  0.14306489196234964 \n",
      "varianza:  0.00028258943762687585\n",
      "c y g:  10 0.1 \n",
      "bias:  0.12800548789799762 \n",
      "varianza:  9.140604703949627e-05\n",
      "c y g:  10 0.01 \n",
      "bias:  0.12694915760005 \n",
      "varianza:  8.798282132084974e-05\n",
      "c y g:  10 0.001 \n",
      "bias:  0.12792769176495347 \n",
      "varianza:  9.924067590581299e-05\n",
      "c y g:  100 0.1 \n",
      "bias:  0.13410231926393607 \n",
      "varianza:  0.00018139133816191558\n",
      "c y g:  100 0.01 \n",
      "bias:  0.13381204473455602 \n",
      "varianza:  0.0001290412579859576\n",
      "c y g:  100 0.001 \n",
      "bias:  0.13167194804635496 \n",
      "varianza:  0.00015124808884962784\n"
     ]
    }
   ],
   "source": [
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results_svm_bootstrap = []\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results_svm_bootstrap.append(bootstrap(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), k, c, gama))\n",
    "\n",
    "# Valores de bias y varianza del respectivo hiperparametro\n",
    "for r in results_svm_bootstrap:\n",
    "    print(\"c y g: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de metricas del mejor modelo SVM con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8730508423999501\n",
      "recall:  0.8730508423999501\n",
      "f1:  0.8730508423999501\n",
      "auc:  0.9452893756425058\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", np.mean(results_svm_bootstrap[4][4]))\n",
    "print(\"recall: \", np.mean(results_svm_bootstrap[4][5]))\n",
    "print(\"f1: \", np.mean(results_svm_bootstrap[4][6]))\n",
    "print(\"auc: \", np.mean(results_svm_bootstrap[4][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = svm.SVC(kernel='linear', C=10, gamma=0.001, decision_function_shape='ovr', probability=True) # C es penalización por los valores fuera de la clasificación\n",
    "# gamma controla la distancia de influencia de un punto de entrenamiento, valor pequeño indica que el radio de distancia es más grande, por lo que mas puntos se agrupan correctamente.\n",
    "# valora alto implica que el radio se reduce y los puntos deben estar más cerca entre ellos para ser considerados del mismo grupo\n",
    "# precisions, recalls, f1s, aucs, errors = k_fold(X, y, clf, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo SVM con K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c y g:  1 0.1 \n",
      "bias:  0.11063567839195979 \n",
      "varianza:  0.00041917338577308693\n",
      "c y g:  1 0.01 \n",
      "bias:  0.11063567839195979 \n",
      "varianza:  0.00041917338577308693\n",
      "c y g:  1 0.001 \n",
      "bias:  0.11063567839195979 \n",
      "varianza:  0.00041917338577308693\n",
      "c y g:  10 0.1 \n",
      "bias:  0.1026256281407035 \n",
      "varianza:  0.000527016773566324\n",
      "c y g:  10 0.01 \n",
      "bias:  0.1026256281407035 \n",
      "varianza:  0.000527016773566324\n",
      "c y g:  10 0.001 \n",
      "bias:  0.1026256281407035 \n",
      "varianza:  0.000527016773566324\n",
      "c y g:  100 0.1 \n",
      "bias:  0.10212311557788945 \n",
      "varianza:  0.0005317254930431063\n",
      "c y g:  100 0.01 \n",
      "bias:  0.10212311557788945 \n",
      "varianza:  0.0005317254930431063\n",
      "c y g:  100 0.001 \n",
      "bias:  0.10212311557788945 \n",
      "varianza:  0.0005317254930431063\n"
     ]
    }
   ],
   "source": [
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results_svm_k = []\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results_svm_k.append(k_fold(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), 10, c, gama))\n",
    "\n",
    "# Valores de bias y varianza del respectivo hiperparametro\n",
    "for r in results_svm_k:\n",
    "    print(\"c y g: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de metricas del mejor modelo SVM con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8973743718592966\n",
      "recall:  0.8973743718592966\n",
      "f1:  0.8973743718592966\n",
      "auc:  0.9695352376362546\n"
     ]
    }
   ],
   "source": [
    "print(\"precision: \", np.mean(results_svm_k[4][4]))\n",
    "print(\"recall: \", np.mean(results_svm_k[4][5]))\n",
    "print(\"f1: \", np.mean(results_svm_k[4][6]))\n",
    "print(\"auc: \", np.mean(results_svm_k[4][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo logística con K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty y c:  l1 10 \n",
      "bias:  0.10611557788944723 \n",
      "varianza:  0.00022118421251988624\n",
      "penalty y c:  l1 1 \n",
      "bias:  0.11162060301507534 \n",
      "varianza:  0.000407358431605263\n",
      "penalty y c:  l1 0.1 \n",
      "bias:  0.13915075376884423 \n",
      "varianza:  0.0005178608873513305\n",
      "penalty y c:  l2 10 \n",
      "bias:  0.10712311557788942 \n",
      "varianza:  0.0003048898070755793\n",
      "penalty y c:  l2 1 \n",
      "bias:  0.11212814070351759 \n",
      "varianza:  0.0005600506363475667\n",
      "penalty y c:  l2 0.1 \n",
      "bias:  0.16668341708542714 \n",
      "varianza:  0.000892777833893083\n"
     ]
    }
   ],
   "source": [
    "results_log_k = []\n",
    "penalty = ['l1', 'l2']\n",
    "cs = [10, 1, 0.1]\n",
    "\n",
    "for p in penalty:\n",
    "    for c in cs:\n",
    "        results_log_k.append(k_fold(X, y, LogisticRegression(penalty=p, C=c, multi_class='ovr', solver='liblinear', max_iter=1000), 10, p, c))\n",
    "\n",
    "for r in results_log_k:\n",
    "   print(\"penalty y c: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de métricas del mejor modelo logística con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8878718592964825\n",
      "recall:  0.8878718592964825\n",
      "f1:  0.8878718592964825\n",
      "auc:  0.9589489224375172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"precision: \", np.mean(results_log_k[4][4]))\n",
    "print(\"recall: \", np.mean(results_log_k[4][5]))\n",
    "print(\"f1: \", np.mean(results_log_k[4][6]))\n",
    "print(\"auc: \", np.mean(results_log_k[4][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo logística con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty y c:  l1 10 \n",
      "bias:  0.12648370788598004 \n",
      "varianza:  7.843229993452602e-05\n",
      "penalty y c:  l1 1 \n",
      "bias:  0.1470087523670004 \n",
      "varianza:  0.00024048535225050662\n",
      "penalty y c:  l1 0.1 \n",
      "bias:  0.22201310672806981 \n",
      "varianza:  9.372234207256527e-06\n",
      "penalty y c:  l2 10 \n",
      "bias:  0.12890514405443643 \n",
      "varianza:  8.421140059238825e-05\n",
      "penalty y c:  l2 1 \n",
      "bias:  0.16991727519998917 \n",
      "varianza:  0.00027923283998021264\n",
      "penalty y c:  l2 0.1 \n",
      "bias:  0.2217022800104486 \n",
      "varianza:  9.786547569525493e-06\n"
     ]
    }
   ],
   "source": [
    "results_log_bootstrap = []\n",
    "penalty = ['l1', 'l2']\n",
    "cs = [10, 1, 0.1]\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for p in penalty:\n",
    "    for c in cs:\n",
    "        results_log_bootstrap.append(bootstrap(X, y, LogisticRegression(penalty=p, C=c, multi_class='ovr', solver='liblinear', max_iter=1000), k, p, c))\n",
    "\n",
    "for r in results_log_bootstrap:\n",
    "   print(\"penalty y c: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
