{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, zero_one_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y normalización de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataset = pd.read_csv('./data/Cardiotocographic-Training.csv')\n",
    "y = dataset.CLASE.to_numpy()\n",
    "X = dataset.drop('CLASE', axis=1).to_numpy()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootsrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y, model, k, c, g):\n",
    "    indices = np.array([i for i in range (len(X))])\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for i in range(k):\n",
    "        train_index = resample(indices, n_samples=k, replace=True)\n",
    "        test_index = np.array([j for j in indices if j not in train_index])\n",
    "        \n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X, y, model, k, c, g):\n",
    "    skf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo de modelo SVM con bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results_svm_bootstrap = []\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results_svm_bootstrap.append(bootstrap(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), k, c, gama))\n",
    "\n",
    "# Valores de bias y varianza del respectivo hiperparametro\n",
    "for r in results_svm_bootstrap:\n",
    "    print(\"c y g: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de metricas del mejor modelo SVM con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precision: \", np.mean(results_svm_bootstrap[4][4]))\n",
    "print(\"recall: \", np.mean(results_svm_bootstrap[4][5]))\n",
    "print(\"f1: \", np.mean(results_svm_bootstrap[4][6]))\n",
    "print(\"auc: \", np.mean(results_svm_bootstrap[4][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = svm.SVC(kernel='linear', C=10, gamma=0.001, decision_function_shape='ovr', probability=True) # C es penalización por los valores fuera de la clasificación\n",
    "# gamma controla la distancia de influencia de un punto de entrenamiento, valor pequeño indica que el radio de distancia es más grande, por lo que mas puntos se agrupan correctamente.\n",
    "# valora alto implica que el radio se reduce y los puntos deben estar más cerca entre ellos para ser considerados del mismo grupo\n",
    "# precisions, recalls, f1s, aucs, errors = k_fold(X, y, clf, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo SVM con K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results_svm_k = []\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results_svm_k.append(k_fold(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), 10, c, gama))\n",
    "\n",
    "# Valores de bias y varianza del respectivo hiperparametro\n",
    "for r in results_svm_k:\n",
    "    print(\"c y g: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de metricas del mejor modelo SVM con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precision: \", np.mean(results_svm_k[4][4]))\n",
    "print(\"recall: \", np.mean(results_svm_k[4][5]))\n",
    "print(\"f1: \", np.mean(results_svm_k[4][6]))\n",
    "print(\"auc: \", np.mean(results_svm_k[4][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo logística con K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_log_k = []\n",
    "penalty = ['l1', 'l2']\n",
    "cs = [10, 1, 0.1]\n",
    "\n",
    "for p in penalty:\n",
    "    for c in cs:\n",
    "        results_log_k.append(k_fold(X, y, LogisticRegression(penalty=p, C=c, multi_class='ovr', solver='liblinear', max_iter=1000), 10, p, c))\n",
    "\n",
    "for r in results_log_k:\n",
    "   print(\"penalty y c: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de métricas del mejor modelo logística con K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"precision: \", np.mean(results_log_k[4][4]))\n",
    "print(\"recall: \", np.mean(results_log_k[4][5]))\n",
    "print(\"f1: \", np.mean(results_log_k[4][6]))\n",
    "print(\"auc: \", np.mean(results_log_k[4][7]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo del modelo logística con Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_log_bootstrap = []\n",
    "penalty = ['l1', 'l2']\n",
    "cs = [10, 1, 0.1]\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for p in penalty:\n",
    "    for c in cs:\n",
    "        results_log_bootstrap.append(bootstrap(X, y, LogisticRegression(penalty=p, C=c, multi_class='ovr', solver='liblinear', max_iter=1000), k, p, c))\n",
    "\n",
    "for r in results_log_bootstrap:\n",
    "   print(\"penalty y c: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
