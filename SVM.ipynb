{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, zero_one_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/Cardiotocographic-Training.csv')\n",
    "y = dataset.CLASE.to_numpy()\n",
    "X = dataset.drop('CLASE', axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y, model, k, c, g):\n",
    "    indices = np.array([i for i in range (len(X))])\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for i in range(k):\n",
    "        train_index = resample(indices, n_samples=k, replace=True)\n",
    "        test_index = np.array([j for j in indices if j not in train_index])\n",
    "        \n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 8 0 4]\n",
      "[0 9 4 1]\n",
      "[9 6 4 8]\n",
      "[9 3 4 8]\n"
     ]
    }
   ],
   "source": [
    "bootstrap(X, y, \"hola\", 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X, y, model, k, c, g):\n",
    "    skf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    aucs = []\n",
    "    errors = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train, y_train = X[train_index], y[train_index]\n",
    "        x_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred_auc = model.predict_proba(x_test)\n",
    "\n",
    "        errors.append(zero_one_loss(y_test, y_pred))\n",
    "\n",
    "        precision = precision_score(y_test, y_pred, average='micro') # micro porque toma en cuenta el desbalanceaminto de clases\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = recall_score(y_test, y_pred, average='micro')\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_pred_auc, multi_class='ovr')\n",
    "        aucs.append(auc)\n",
    "    return [np.mean(errors), np.var(errors), c, g, precisions, recalls, f1s, aucs, errors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m Cs:\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m gama \u001b[39min\u001b[39;00m gammas:\n\u001b[0;32m---> 10\u001b[0m         results\u001b[39m.\u001b[39mappend(bootstrap(X, y, svm\u001b[39m.\u001b[39;49mSVC(kernel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m, C\u001b[39m=\u001b[39;49mc, gamma\u001b[39m=\u001b[39;49mgama, decision_function_shape\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39movr\u001b[39;49m\u001b[39m'\u001b[39;49m, probability\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), k, c, gama))\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m results:\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbias: \u001b[39m\u001b[39m\"\u001b[39m, r[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mvarianza: \u001b[39m\u001b[39m\"\u001b[39m, r[\u001b[39m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[61], line 15\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m(X, y, model, k, c, g)\u001b[0m\n\u001b[1;32m     12\u001b[0m x_train, y_train \u001b[39m=\u001b[39m X[train_index], y[train_index]\n\u001b[1;32m     13\u001b[0m x_test, y_test \u001b[39m=\u001b[39m X[test_index], y[test_index]\n\u001b[0;32m---> 15\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[1;32m     16\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[1;32m     17\u001b[0m y_pred_auc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(x_test)\n",
      "File \u001b[0;32m~/IA/IA-Proyecto1/env/lib/python3.10/site-packages/sklearn/svm/_base.py:252\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[0;32m--> 252\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[1;32m    253\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m~/IA/IA-Proyecto1/env/lib/python3.10/site-packages/sklearn/svm/_base.py:331\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    317\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    319\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    321\u001b[0m (\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[1;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_,\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[1;32m    330\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_iter,\n\u001b[0;32m--> 331\u001b[0m ) \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    332\u001b[0m     X,\n\u001b[1;32m    333\u001b[0m     y,\n\u001b[1;32m    334\u001b[0m     svm_type\u001b[39m=\u001b[39;49msolver_type,\n\u001b[1;32m    335\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    336\u001b[0m     \u001b[39m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    337\u001b[0m     class_weight\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_class_weight\u001b[39;49m\u001b[39m\"\u001b[39;49m, np\u001b[39m.\u001b[39;49mempty(\u001b[39m0\u001b[39;49m)),\n\u001b[1;32m    338\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[1;32m    339\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m    340\u001b[0m     nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[1;32m    341\u001b[0m     probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability,\n\u001b[1;32m    342\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[1;32m    343\u001b[0m     shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking,\n\u001b[1;32m    344\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    345\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[1;32m    346\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[1;32m    347\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[1;32m    348\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[1;32m    349\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    350\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[1;32m    351\u001b[0m )\n\u001b[1;32m    353\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results = []\n",
    "\n",
    "k = int(len(X)/10)\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results.append(bootstrap(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), k, c, gama))\n",
    "\n",
    "for r in results:\n",
    "    print(\"bias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias:  0.09811809045226129 \n",
      "varianza:  0.0003309761685310984\n",
      "bias:  0.09811809045226129 \n",
      "varianza:  0.0003309761685310984\n",
      "bias:  0.09811809045226129 \n",
      "varianza:  0.0003309761685310984\n",
      "bias:  0.09311055276381906 \n",
      "varianza:  0.0002550091916870789\n",
      "bias:  0.09311055276381906 \n",
      "varianza:  0.0002550091916870789\n",
      "bias:  0.09311055276381906 \n",
      "varianza:  0.0002550091916870789\n",
      "bias:  0.10111557788944721 \n",
      "varianza:  0.00037274301154011335\n",
      "bias:  0.10111557788944721 \n",
      "varianza:  0.00037274301154011335\n",
      "bias:  0.10111557788944721 \n",
      "varianza:  0.00037274301154011335\n"
     ]
    }
   ],
   "source": [
    "#clf = svm.SVC(kernel='linear', C=10, gamma=0.001, decision_function_shape='ovr', probability=True) # C es penalización por los valores fuera de la clasificación\n",
    "# gamma controla la distancia de influencia de un punto de entrenamiento, valor pequeño indica que el radio de distancia es más grande, por lo que mas puntos se agrupan correctamente.\n",
    "# valora alto implica que el radio se reduce y los puntos deben estar más cerca entre ellos para ser considerados del mismo grupo\n",
    "#precisions, recalls, f1s, aucs, errors = k_fold(X, y, clf, 10)\n",
    "\n",
    "Cs = [1, 10, 100]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "results = []\n",
    "\n",
    "for c in Cs:\n",
    "    for gama in gammas:\n",
    "        results.append(k_fold(X, y, svm.SVC(kernel='linear', C=c, gamma=gama, decision_function_shape='ovr', probability=True), 10, c, gama))\n",
    "\n",
    "for r in results:\n",
    "    print(\"bias: \", r[0], \"\\nvarianza: \", r[1])\n",
    "\n",
    "# El menor bias y varianza será el que nos de el modelo con menor overfitting\n",
    "\n",
    "# clf = svm.SVC(kernel='linear') # 0.91166666\n",
    "# clf.fit(X_train, y_train)\n",
    "# print(clf.class_weight_)\n",
    "# print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.906889447236181\n",
      "recall:  0.906889447236181\n",
      "f1:  0.906889447236181\n",
      "auc:  0.9686436003609835\n",
      "[0.06999999999999995, 0.08499999999999996, 0.08999999999999997, 0.07499999999999996, 0.09499999999999997, 0.08499999999999996, 0.08999999999999997, 0.12, 0.12060301507537685, 0.10050251256281406]\n"
     ]
    }
   ],
   "source": [
    "#for r in results:\n",
    "#    print(\"c y g: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])\n",
    "\n",
    "print(\"precision: \", np.mean(results[4][4]))\n",
    "print(\"recall: \", np.mean(results[4][5]))\n",
    "print(\"f1: \", np.mean(results[4][6]))\n",
    "print(\"auc: \", np.mean(results[4][7]))\n",
    "print(results[4][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias:  0.10661557788944723 \n",
      "varianza:  0.00025731863463043905\n",
      "bias:  0.10611306532663316 \n",
      "varianza:  0.00020053357869750794\n",
      "bias:  0.11512311557788943 \n",
      "varianza:  0.00013202071286078657\n",
      "bias:  0.10611557788944723 \n",
      "varianza:  0.00023618421251988625\n",
      "bias:  0.10762311557788942 \n",
      "varianza:  0.00035001669149769003\n",
      "bias:  0.11512814070351758 \n",
      "varianza:  0.0001982817921264618\n"
     ]
    }
   ],
   "source": [
    "results2 = []\n",
    "penalty = ['l1', 'l2']\n",
    "cs = [10, 1, 0.1]\n",
    "\n",
    "for p in penalty:\n",
    "    for c in cs:\n",
    "        results2.append(k_fold(X, y, LogisticRegression(penalty=p, C=c, multi_class='ovr', solver='liblinear', max_iter=1000), 10, p, c))\n",
    "\n",
    "for r in results2:\n",
    "    print(\"bias: \", r[0], \"\\nvarianza: \", r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty y c:  l1 10 \n",
      "bias:  0.10661557788944723 \n",
      "varianza:  0.00025731863463043905\n",
      "penalty y c:  l1 1 \n",
      "bias:  0.10611306532663316 \n",
      "varianza:  0.00020053357869750794\n",
      "penalty y c:  l1 0.1 \n",
      "bias:  0.11512311557788943 \n",
      "varianza:  0.00013202071286078657\n",
      "penalty y c:  l2 10 \n",
      "bias:  0.10611557788944723 \n",
      "varianza:  0.00023618421251988625\n",
      "penalty y c:  l2 1 \n",
      "bias:  0.10762311557788942 \n",
      "varianza:  0.00035001669149769003\n",
      "penalty y c:  l2 0.1 \n",
      "bias:  0.11512814070351758 \n",
      "varianza:  0.0001982817921264618\n",
      "precision:  0.8923768844221106\n",
      "recall:  0.8923768844221106\n",
      "f1:  0.8923768844221106\n",
      "auc:  0.9624252699845126\n"
     ]
    }
   ],
   "source": [
    "for r in results2:\n",
    "   print(\"penalty y c: \", r[2], r[3], \"\\nbias: \", r[0], \"\\nvarianza: \", r[1])\n",
    "print(\"precision: \", np.mean(results2[4][4]))\n",
    "print(\"recall: \", np.mean(results2[4][5]))\n",
    "print(\"f1: \", np.mean(results2[4][6]))\n",
    "print(\"auc: \", np.mean(results2[4][7]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
